{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent and Variants\n",
    "- Import all the required libraries for the implementation.\n",
    "- Used 42 as the random seed for the entire notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Algorithm\n",
    "- Implemented a class for the Rosenbrock function.\n",
    "- Created different function for each variant of the Gradient Descent.\n",
    "- Using 1e4 as the number of iterations for each of the algorithm.\n",
    "- Used 1e-06 as the tolerance to check the convergence of each algorithm.\n",
    "- Implemented functions to find the function value and its gradient at any point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rosenbrock:\n",
    "    def f(self, x, y):\n",
    "        return x**2 + 100*(y-x**2)**2\n",
    "\n",
    "    def df(self, x, y):\n",
    "        df_dx = 2*x - 400*x*(y-x**2)\n",
    "        df_dy = 200*(y-x**2)\n",
    "        return np.array([df_dx, df_dy])\n",
    "\n",
    "    def gradient_descent(self, start_x, start_y, learn_rate, n_iter, tolerance=1e-06):\n",
    "        curr = np.array([start_x, start_y])\n",
    "        path = [curr]\n",
    "        for _ in range(n_iter):\n",
    "            gradient = self.df(*curr)\n",
    "            diff = -learn_rate*gradient\n",
    "            curr = curr + diff\n",
    "            path.append(curr)\n",
    "            if np.linalg.norm(gradient) < tolerance:\n",
    "                break\n",
    "        return path\n",
    "\n",
    "    def gradient_descent_polyak(self, start_x, start_y, learn_rate, n_iter, momentum, tolerance=1e-06):\n",
    "        curr = np.array([start_x, start_y])\n",
    "        update = np.zeros_like(curr)\n",
    "        path = [curr]\n",
    "        for _ in range(n_iter):\n",
    "            gradient = self.df(*curr)\n",
    "            update = momentum*update + learn_rate*gradient\n",
    "            curr = curr - update\n",
    "            path.append(curr)\n",
    "            if np.linalg.norm(gradient) < tolerance:\n",
    "                break\n",
    "        return path\n",
    "\n",
    "    def gradient_descent_nesterov(self, start_x, start_y, learn_rate, n_iter, momentum, tolerance=1e-06):\n",
    "        curr = np.array([start_x, start_y])\n",
    "        update = np.zeros_like(curr)\n",
    "        path = [curr]\n",
    "        for _ in range(n_iter):\n",
    "            temp = curr - momentum*update\n",
    "            gradient = self.df(*temp)\n",
    "            update = momentum*update + learn_rate*gradient\n",
    "            curr = curr - update\n",
    "            path.append(curr)\n",
    "            if np.linalg.norm(gradient) < tolerance:\n",
    "                break\n",
    "        return path\n",
    "\n",
    "    def gradient_descent_adam(self, start_x, start_y, learn_rate, n_iter, gamma, delta, tolerance=1e-06):\n",
    "        epsilon = 1e-08\n",
    "        curr = np.array([start_x, start_y])\n",
    "        update = np.zeros_like(curr)\n",
    "        temp = np.zeros_like(curr)\n",
    "        path = [curr]\n",
    "        t = 0\n",
    "        for _ in range(n_iter):\n",
    "            t += 1\n",
    "            gradient = self.df(*curr)\n",
    "            update = delta*update + (1-delta)*gradient\n",
    "            temp = gamma*temp + (1-gamma)*(gradient**2)\n",
    "            updateL = update / (1-delta**t)\n",
    "            tempL = temp / (1-gamma**t)\n",
    "            curr = curr - learn_rate * updateL / (np.sqrt(tempL) + epsilon)\n",
    "            path.append(curr)\n",
    "            if np.linalg.norm(gradient) < tolerance:\n",
    "                break\n",
    "        return path\n",
    "\n",
    "    def contour_plot(self, path, title):\n",
    "        x = np.linspace(-1, 1, 100)\n",
    "        y = np.linspace(-1, 1, 100)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        Z = self.f(X, Y)\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.contourf(X, Y, Z, levels=100, cmap='viridis')\n",
    "        plt.colorbar(label='Function Value')\n",
    "\n",
    "        x_traj, y_traj = zip(*path)\n",
    "        plt.quiver(x_traj[:-1], y_traj[:-1], np.diff(x_traj), np.diff(y_traj), scale_units='xy', angles='xy', scale=1, color='red', label='Gradient Descent Path')\n",
    "        plt.scatter(0, 0, color='yellow', marker='x', s=100, label='Global Minimum (0, 0)')\n",
    "\n",
    "        plt.xlabel('X')\n",
    "        plt.ylabel('Y')\n",
    "        plt.title(title)\n",
    "\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the algorithm\n",
    "- Randomly choose the starting point between -1 and 1.\n",
    "- Used 0.001 as the learning rate and 0.9 as momentum for Polyak's momentum method.\n",
    "- Used 0.9 as the momentum for Nesterov accelerated gradient descent.\n",
    "- used 0.999 as gamma value and 0.9 as the delta value for the Adam optimizer.\n",
    "- Plotted the contour plot using arrow to show the movement after each update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 1: Rosenbrock function\n",
    "x = np.random.uniform(-1, 1)\n",
    "y = np.random.uniform(-1, 1)\n",
    "learn_rate = 0.001\n",
    "n_iter = 10000\n",
    "momentum = 0.9\n",
    "gamma = 0.999\n",
    "delta = 0.9\n",
    "\n",
    "titles = [\"Gradient Descent\", \"Gradient Descent with Polyak's momentum\", \"Nesterov accelerated gradient descent\", \"Gradient Descent using Adam Optimizer\"]\n",
    "rose = Rosenbrock()\n",
    "rose.contour_plot(rose.gradient_descent(x, y, learn_rate, n_iter), titles[0])\n",
    "rose.contour_plot(rose.gradient_descent_polyak(x, y, learn_rate, n_iter, momentum), titles[1])\n",
    "rose.contour_plot(rose.gradient_descent_nesterov(x, y, learn_rate, n_iter, momentum), titles[2])\n",
    "rose.contour_plot(rose.gradient_descent_adam(x, y, learn_rate, n_iter, gamma, delta), titles[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROSENBROCK FUNCTION f(x, y) = x^2 + 100(y-x^2)^2\n",
    "![Gradient Descent](RGD.png)\n",
    "![Gradient Descent with Polyak's momentum](RGDP.png)\n",
    "![Nesterov Gradient Descent](RGDN.png)\n",
    "![Gradient Descent with Adam Optimizer](RGDA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Algorithm\n",
    "- Implemented a class for the given function.\n",
    "- Created different function for each variant of the Gradient Descent.\n",
    "- Using 1e4 as the number of iterations for each of the algorithm.\n",
    "- Used 1e-06 as the tolerance to check the convergence of each algorithm.\n",
    "- Implemented functions to find the function value and its gradient at any point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Function:\n",
    "    def f(self, x, y):\n",
    "        return (50/9)*(x**2+y**2)**3 - (209/18)*(x**2+y**2)**2 + (59/9)*(x**2+y**2)\n",
    "\n",
    "    def df(self, x, y):\n",
    "        df_dx = (100/3)*x*(x**2+y**2)**2 - (418/9)*x*(x**2+y**2) + (118/9)*x\n",
    "        df_dy = (100/3)*y*(x**2+y**2)**2 - (418/9)*y*(x**2+y**2) + (118/9)*y\n",
    "        return np.array([df_dx, df_dy])\n",
    "\n",
    "    def gradient_descent(self, start_x, start_y, learn_rate, n_iter, tolerance=1e-06):\n",
    "        curr = np.array([start_x, start_y])\n",
    "        path = [curr]\n",
    "        for _ in range(n_iter):\n",
    "            gradient = self.df(*curr)\n",
    "            diff = -learn_rate*gradient\n",
    "            curr = curr + diff\n",
    "            path.append(curr)\n",
    "            if np.linalg.norm(gradient) < tolerance:\n",
    "                break\n",
    "        return path\n",
    "\n",
    "    def gradient_descent_polyak(self, start_x, start_y, learn_rate, n_iter, momentum, tolerance=1e-06):\n",
    "        curr = np.array([start_x, start_y])\n",
    "        update = np.zeros_like(curr)\n",
    "        path = [curr]\n",
    "        for _ in range(n_iter):\n",
    "            gradient = self.df(*curr)\n",
    "            update = momentum*update + learn_rate*gradient\n",
    "            curr = curr - update\n",
    "            path.append(curr)\n",
    "            if np.linalg.norm(gradient) < tolerance:\n",
    "                break\n",
    "        return path\n",
    "\n",
    "    def gradient_descent_nesterov(self, start_x, start_y, learn_rate, n_iter, momentum, tolerance=1e-06):\n",
    "        curr = np.array([start_x, start_y])\n",
    "        update = np.zeros_like(curr)\n",
    "        path = [curr]\n",
    "        for _ in range(n_iter):\n",
    "            temp = curr - momentum*update\n",
    "            gradient = self.df(*temp)\n",
    "            update = momentum*update + learn_rate*gradient\n",
    "            curr = curr - update\n",
    "            path.append(curr)\n",
    "            if np.linalg.norm(gradient) < tolerance:\n",
    "                break\n",
    "        return path\n",
    "\n",
    "    def gradient_descent_adam(self, start_x, start_y, learn_rate, n_iter, gamma, delta, tolerance=1e-06):\n",
    "        epsilon = 1e-08\n",
    "        curr = np.array([start_x, start_y])\n",
    "        update = np.zeros_like(curr)\n",
    "        temp = np.zeros_like(curr)\n",
    "        path = [curr]\n",
    "        t = 0\n",
    "        for _ in range(n_iter):\n",
    "            t += 1\n",
    "            gradient = self.df(*curr)\n",
    "            update = delta*update + (1-delta)*gradient\n",
    "            temp = gamma*temp + (1-gamma)*(gradient**2)\n",
    "            updateL = update / (1-delta**t)\n",
    "            tempL = temp / (1-gamma**t)\n",
    "            curr = curr - learn_rate * updateL / (np.sqrt(tempL) + epsilon)\n",
    "            path.append(curr)\n",
    "            if np.linalg.norm(gradient) < tolerance:\n",
    "                break\n",
    "        return path\n",
    "\n",
    "    def contour_plot(self, path, title):\n",
    "        x = np.linspace(-1, 1, 100)\n",
    "        y = np.linspace(-1, 1, 100)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        Z = self.f(X, Y)\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.contourf(X, Y, Z, levels=100, cmap='viridis')\n",
    "        plt.colorbar(label='Function Value')\n",
    "\n",
    "        x_traj, y_traj = zip(*path)\n",
    "        plt.quiver(x_traj[:-1], y_traj[:-1], np.diff(x_traj), np.diff(y_traj), scale_units='xy', angles='xy', scale=1, color='red', label='Gradient Descent Path')\n",
    "        plt.scatter(0, 0, color='yellow', marker='x', s=100, label='Global Minimum (0, 0)')\n",
    "\n",
    "        plt.xlabel('X')\n",
    "        plt.ylabel('Y')\n",
    "        plt.title(title)\n",
    "\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the algorithm\n",
    "- Randomly choose the starting point between -1 and 1.\n",
    "- Used 0.001 as the learning rate and 0.9 as momentum for Polyak's momentum method.\n",
    "- Used 0.9 as the momentum for Nesterov accelerated gradient descent.\n",
    "- used 0.999 as gamma value and 0.9 as the delta value for the Adam optimizer.\n",
    "- Plotted the contour plot using arrow to show the movement after each update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 2: f(x, y) = (50/9)(x^2 + y^2)^3 − (209/18)(x^2 + y^2)^2 + (59/9)(x^2 + y^2)\n",
    "x = np.random.uniform(-1, 1)\n",
    "y = np.random.uniform(-1, 1)\n",
    "learn_rate = 0.01\n",
    "n_iter = 10000\n",
    "momentum = 0.9\n",
    "gamma = 0.999\n",
    "delta = 0.9\n",
    "\n",
    "titles = [\"Gradient Descent\", \"Gradient Descent with Polyak's momentum\", \"Nesterov accelerated gradient descent\", \"Gradient Descent using Adam Optimizer\"]\n",
    "func = Function()\n",
    "func.contour_plot(func.gradient_descent(x, y, learn_rate, n_iter), titles[0])\n",
    "func.contour_plot(func.gradient_descent_polyak(x, y, learn_rate, n_iter, momentum), titles[1])\n",
    "func.contour_plot(func.gradient_descent_nesterov(x, y, learn_rate, n_iter, momentum), titles[2])\n",
    "func.contour_plot(func.gradient_descent_adam(x, y, learn_rate, n_iter, gamma, delta), titles[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTION f(x, y) = (50/9)(x^2 + y^2)^3 − (209/18)(x^2 + y^2)^2 + (59/9)(x^2 + y^2)\n",
    "![Gradient Descent](FGD.png)\n",
    "![Gradient Descent with Polyak's momentum](FGDP.png)\n",
    "![Nesterov Gradient Descent](FGDN.png)\n",
    "![Gradient Descent with Adam Optimizer](FGDA.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
