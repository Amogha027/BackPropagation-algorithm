{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propagation, Resilient Propagation (RProp) and QuickProp\n",
    "- Import all the required libraries for the implementation.\n",
    "- Used 42 as the random seed for the entire notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data and Data preprocessing\n",
    "- Downloaded the Concrete Compressive Strength dataset from the link below.\n",
    "- Link: https://archive.ics.uci.edu/dataset/165/concrete+compressive+strength\n",
    "- Extracted the zip file and placed the Concret_Data.xls file in the directory containing the notebook.\n",
    "- Loaded the data form the file and split the data into train and test data with train size = 0.7\n",
    "- Both the dependent and independent data are normalized to make sure data lies within a given range.\n",
    "- Normalization was done in order to avoid the integer overflow during the weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('Concrete_Data.xls')\n",
    "X = data.iloc[:,0:8]\n",
    "Y = data.iloc[:,8]\n",
    "columns = list(X)\n",
    "for i in columns:\n",
    "    X[i] = (X[i] - min(X[i])) / (max(X[i]) - min(X[i]))\n",
    "Y = (Y - min(Y)) / (max(Y) - min(Y))\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Algorithm\n",
    "- Created a class which takes algorithm, number of layers and activation function as parameters.\n",
    "- The weights, delta weights and previous gradients are intialised randomly in the class.\n",
    "- Used Mean Squared Error(MSE) as the loss function and 0.001 as the learning rate.\n",
    "- Each network is trained for 1e3 epochs irrespective of the convergence is achieved or not.\n",
    "- Implemented separate methods to update weights incase of BackProp, RProp and QuickProp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, algo, layers, activation, rate=0.001):\n",
    "        self.weights = []\n",
    "        self.algo = algo\n",
    "        self.layers = layers\n",
    "        self.func = activation\n",
    "        self.rate = rate\n",
    "        self.train = []\n",
    "        self.test = []\n",
    "\n",
    "        if algo == 'RProp':\n",
    "            for i in range(len(layers)-2):\n",
    "                w = np.random.rand(layers[i]+1, layers[i+1]+1)\n",
    "                self.weights.append(w / np.sqrt(layers[i]))\n",
    "            w = np.random.rand(layers[-2]+1, layers[-1])\n",
    "            self.weights.append(w / np.sqrt(layers[-2]))\n",
    "            self.prev = [rate*np.ones_like(w) for w in self.weights]\n",
    "            self.derivatives = [np.zeros_like(w) for w in self.weights]\n",
    "        else:\n",
    "            for i in range(len(layers)-2):\n",
    "                w = np.random.randn(layers[i]+1, layers[i+1]+1)\n",
    "                self.weights.append(w / np.sqrt(layers[i]))\n",
    "            w = np.random.randn(layers[-2]+1, layers[-1])\n",
    "            self.weights.append(w / np.sqrt(layers[-2]))\n",
    "            if algo == 'QuickProp':\n",
    "                self.prev = [rate*np.ones_like(w) for w in self.weights]\n",
    "                self.derivatives = [np.zeros_like(w) for w in self.weights]\n",
    "\n",
    "    def activation(self, x):\n",
    "        if self.func == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        if self.func == 'LeakyReLU':\n",
    "            return np.where(x>0, x, 0.01*x)\n",
    "        \n",
    "    def activation_gradient(self, x):\n",
    "        if self.func == 'tanh':\n",
    "            return 1-self.activation(x)**2\n",
    "        if self.func == 'LeakyReLU':\n",
    "            return np.where(x>0, 1, 0.01)\n",
    "    \n",
    "    def calculate_loss(self, y_test, y_pred):\n",
    "        y_test = np.atleast_2d(y_test)\n",
    "        y_pred = np.atleast_2d(y_pred)\n",
    "        mse = np.mean((y_pred-y_test)**2)\n",
    "        return mse\n",
    "        \n",
    "    def predict(self, x, y):\n",
    "        values = [np.atleast_2d(x)]\n",
    "        for layer in range(len(self.weights)):\n",
    "            z = np.dot(values[layer], self.weights[layer])\n",
    "            values.append(self.activation(z))\n",
    "        loss = self.calculate_loss(y, values[-1])\n",
    "        return loss\n",
    "        \n",
    "    def fit(self, x_train, y_train, x_test, y_test, epochs=1e3):\n",
    "        x_train = np.c_[x_train, np.ones((x_train.shape[0]))]\n",
    "        x_test = np.c_[x_test, np.ones((x_test.shape[0]))]\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, test_loss = 0, 0\n",
    "            for (x, y) in zip(x_train, y_train):\n",
    "                train_loss += self.fit_instant(x, y)\n",
    "            train_loss = train_loss / len(x_train)\n",
    "            self.train.append(train_loss)\n",
    "\n",
    "            for (x, y) in zip(x_test, y_test):\n",
    "                test_loss += self.predict(x, y)\n",
    "            test_loss = test_loss / len(x_test)\n",
    "            self.test.append(test_loss)\n",
    "            # print(f'epoch: {1+epoch}, train_error: {train_loss}, test_error: {test_loss}')\n",
    "\n",
    "    def fit_instant(self, x, y):\n",
    "        values = [np.atleast_2d(x)]\n",
    "        # forward pass\n",
    "        for layer in range(len(self.weights)):\n",
    "            z = np.dot(values[layer], self.weights[layer])\n",
    "            values.append(self.activation(z))\n",
    "        loss = self.calculate_loss(y, values[-1])\n",
    "\n",
    "        # backward pass\n",
    "        error = values[-1]-y\n",
    "        derivatives = [error*self.activation_gradient(values[-1])]\n",
    "        for layer in range(len(self.weights)-1, 0, -1):\n",
    "            delta = np.dot(derivatives[-1], self.weights[layer].T)\n",
    "            delta = delta*self.activation_gradient(values[layer])\n",
    "            derivatives.append(delta)\n",
    "        derivatives = derivatives[::-1]\n",
    "            \n",
    "        # update the weights\n",
    "        if self.algo == 'BackProp':\n",
    "            for layer in range(len(self.weights)):\n",
    "                self.weights[layer] += -self.rate*np.dot(values[layer].T, derivatives[layer])\n",
    "\n",
    "        if self.algo == 'RProp':\n",
    "            alpha = 1.2\n",
    "            beta = 0.5\n",
    "            for layer in range(len(self.weights)):\n",
    "                gradient = np.dot(values[layer].T, derivatives[layer])\n",
    "                sign = np.sign(gradient)*np.sign(self.derivatives[layer])\n",
    "                delta = self.prev[layer]\n",
    "                delta[sign>0] *= alpha\n",
    "                delta[sign<0] *= beta\n",
    "                delta = np.clip(delta, 1e-06, 50)\n",
    "                self.prev[layer] = delta\n",
    "                self.weights[layer] += -sign*delta\n",
    "                self.derivatives[layer] = derivatives[layer]\n",
    "\n",
    "        if self.algo == 'QuickProp':\n",
    "            for layer in range(len(self.weights)):\n",
    "                delta = (self.prev[layer]*derivatives[layer]) / (self.derivatives[layer]-derivatives[layer]+1e-9)\n",
    "                delta = np.clip(delta, 1e-06, 50)\n",
    "                self.derivatives[layer] = derivatives[layer]\n",
    "                self.weights[layer] += self.rate*delta\n",
    "                self.prev[layer] = delta\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    def calculate(self):\n",
    "        threshold = 1e-3\n",
    "        loss = self.test[-1]\n",
    "        for epoch, mse in enumerate(self.test):\n",
    "            if epoch > 0 and abs(mse-self.test[epoch-1]) < threshold:\n",
    "                return loss, epoch\n",
    "        return loss, 1000\n",
    "    \n",
    "    def lowestMSE(self):\n",
    "        return min(self.train), min(self.test)\n",
    "    \n",
    "    def plot(self, title):\n",
    "        idx = [i+1 for i in range(1000)]\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(idx, self.train, 'b', label='train data')\n",
    "        plt.plot(idx, self.test, 'r', label='test data')\n",
    "        plt.xlabel('Number of epochs')\n",
    "        plt.ylabel('MSE')\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Algorithm\n",
    "- Used BackPropagation, Resilient Propagation and QuickProp as the three algorithms.\n",
    "- Used tanh, LeakyReLU as the activation functions and 25 or 50 hidden units in each network.\n",
    "- Trained each of the triplet (algorithm, number of layers, activation function).\n",
    "- Graphs of MSE vs number of epochs is plotted for train and test data for each network.\n",
    "- Also, indicated the loss, epochs to convergence, time for each network in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = ['BackProp', 'QuickProp', 'RProp']\n",
    "activations = ['tanh', 'LeakyReLU']\n",
    "layers = [[8, 25, 1], [8, 50, 1]]\n",
    "\n",
    "results = []\n",
    "for algo in algos:\n",
    "    for layer in layers:\n",
    "        for activation in activations:\n",
    "            network = Network(algo, layer, activation)\n",
    "            start = time.time()\n",
    "            network.fit(x_train, y_train, x_test, y_test, 1000)\n",
    "            end = time.time()\n",
    "            title = f'Plot for {algo} using {activation} ({layer[1]} hidden units)'\n",
    "            network.plot(title)\n",
    "            l1, l2 = network.lowestMSE()\n",
    "            loss, epochs = network.calculate()\n",
    "            results.append([algo, layer[1], activation, loss, epochs, end-start, l1, l2])\n",
    "\n",
    "headers = ['Network', 'Hidden Units', 'Activation', 'loss', 'epochs to convergence', 'time', 'lowest train MSE', 'lowest test MSE']\n",
    "print(tabulate(results, headers=headers, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Results\n",
    "- From the data obtained we can conclude that BackProp takes less time and is optimized algorithm.\n",
    "- Using 50 hidden layers leads to earlier convergence as compared to 25 hidden layers.\n",
    "- Leaky ReLU has lower loss in BackProp and QuickProp whereas tanh has lower loss in case of RProp.\n",
    "- So, the choice of activation function depends on specific problem and the architecture of the network.\n",
    "\n",
    "![BackPropagation with tanh (25 hidden units)](BPtanh25.png)\n",
    "![BackPropagation with tanh (50 hidden units)](BPtanh50.png)\n",
    "![BackPropagation with Leaky ReLU (25 hidden units)](BPLReLU25.png)\n",
    "![BackPropagation with Leaky ReLU (50 hidden units)](BPLReLU50.png)\n",
    "![QuickPropagation with tanh (25 hidden units)](QPtanh25.png)\n",
    "![QuickPropagation with tanh (50 hidden units)](QPtanh50.png)\n",
    "![QuickPropagation with Leaky ReLU (25 hidden units)](QPLReLU25.png)\n",
    "![QuickPropagation with Leaky ReLU (50 hidden units)](QPLReLU50.png)\n",
    "![RPropagation with tanh (25 hidden units)](RPtanh25.png)\n",
    "![RPropagation with tanh (50 hidden units)](RPtanh50.png)\n",
    "![RPropagation with Leaky ReLU (25 hidden units)](RPLReLU25.png)\n",
    "![RPropagation with Leaky ReLU (50 hidden units)](RPLReLU50.png)\n",
    "![Data Analysis](data.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
